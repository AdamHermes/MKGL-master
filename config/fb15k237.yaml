output_dir: &output_dir outputs/

model_name: &model_name TinyLlama/TinyLlama-1.1B-Chat-v1.0
kgl_token_length: 10

dataset:
  class: FB15k237
  path: data/datasets/

tokenizer:
  pretrained_model_name_or_path: *model_name
  use_fast: no
  add_eos_token: no

mkglconfig:
  pretrained_model_name_or_path: *model_name

mkgl:
  pretrained_model_name_or_path: *model_name
  load_in_8bit: no

loraconfig:
  r: &r 32
  lora_alpha: 8
  lora_dropout: 0.05
  target_modules:
    # - embed_tokens
    # - lm_head
    - q_proj
    - v_proj

trainer:
  output_dir: *output_dir
  num_train_epochs: 5
  save_total_limit: 1
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 16
  eval_strategy: epoch
  eval_steps: 500
  save_strategy: "no" #epoch
  warmup_steps: 50
  bf16: yes
  logging_steps: 10
  logging_strategy: steps
  learning_rate: 5.0e-3
  gradient_accumulation_steps: 1
  eval_accumulation_steps: 64
  save_safetensors: no
  remove_unused_columns: no
  label_names:
    - label
  optim: adamw_8bit
  max_grad_norm: 1.
  ddp_find_unused_parameters: yes
  report_to:
    - tensorboard

mkgl4kgc:
  criterion: bce
  num_negative: 32
  strict_negative: yes
  adversarial_temperature: 1

context_retriever:
  llm_hidden_dim: &llm_hidden_dim 2048
  r: *r
  text_encoder: pna
  kg_encoder:
    in_dim: *r
    out_dim: *r
    num_relations: 472
    hidden_dim: 256
    num_layers: 1
    remove_one_hop: yes
    node_ratio: 0.1

score_retriever:
  llm_hidden_dim: *llm_hidden_dim
  r: *r
  text_encoder: pna
  kg_encoder:
    in_dim: *r # input feature dimension
    out_dim: *r # output embedding dimension
    num_relations: 472 # number of relations in FB15k-237
    hidden_dim: 256 # hidden dim inside ConditionedPNA MLP (optional)
    num_layers: 2 # corresponds to TorchDrug num_layer
    remove_one_hop: yes # keep same
    node_ratio: 0.1 # same as TorchDrug